{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO8vbqBUqknZOyXza+XbaQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iremaricii/Metin_Tabanli_Goruntu_Uretimi-FineTunning/blob/main/Metin_Tabanli_Goruntu_Uretimi_FineTunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rHsrImMBcIc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers accelerate scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow"
      ],
      "metadata": {
        "id": "vTl50LYAB3q9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "\n",
        "# GPU kullanımı için cihazı belirleme\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Modelin Hugging Face üzerinden yüklenmesi\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# Örnek bir prompt ile görsel üretimi\n",
        "prompt = \"a portrait of a person\"\n",
        "result = pipe(prompt)\n",
        "image = result.images[0]\n",
        "\n",
        "# Üretilen görseli kaydetme ve görüntüleme\n",
        "image.save(\"portraitex.png\")\n",
        "image"
      ],
      "metadata": {
        "id": "u1A7LGa4B_DQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kS9uBMFdCJ_0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/archive (1).zip\" -d \"/content/dataset\""
      ],
      "metadata": {
        "id": "8DBVK2IWCQB3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/dataset\""
      ],
      "metadata": {
        "id": "TmLKbEk6CYqZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/drive -type f -name \"*.zip\""
      ],
      "metadata": {
        "id": "nP9TGl8kCa45",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "CX5i9jDyCsiA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "# Pipeline'ı yükleyelim ve GPU'ya taşıyalım\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "kGp3n7KYC0PR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğitim tamamlandıktan sonra, text encoder'ı FP16'ya dönüştürelim:\n",
        "with torch.no_grad():\n",
        "    pipe.text_encoder.half()\n",
        "\n",
        "# 8. Eğitilmiş Token'ı Test Etme\n",
        "pipe.unet.eval()\n",
        "test_prompt = f\"a photo of {new_token}\"\n",
        "result = pipe(test_prompt, guidance_scale=7.5)\n",
        "result.images[0]"
      ],
      "metadata": {
        "id": "kBD58zsBC6Yz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from PIL import Image\n",
        "import glob\n",
        "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
        "\n",
        "# 1. Pipeline ve Model Bileşenlerinin Yüklenmesi\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "# Modeli fp16 ile yükleyip GPU'ya taşıyalım\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# UNet ve VAE parametrelerini donduralım\n",
        "for param in pipe.unet.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in pipe.vae.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Text encoder’ın tüm parametrelerini donduralım\n",
        "for param in pipe.text_encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Ancak, input embedding katmanını açalım\n",
        "embedding_layer = pipe.text_encoder.get_input_embeddings()\n",
        "embedding_layer.weight.requires_grad = True\n",
        "# Yeni token eklendikten sonra, input embedding weight’ini FP32’ye çeviriyoruz\n",
        "embedding_layer.weight.data = embedding_layer.weight.data.float()\n",
        "\n",
        "# 2. Yeni Token'ı Ekleyip, Text Encoder Embedding'ine Entegre Edelim\n",
        "new_token = \"<my_concept>\"  # Öğrenmesini istediğiniz konsept token'ı\n",
        "num_added_tokens = pipe.tokenizer.add_tokens(new_token)\n",
        "if num_added_tokens > 0:\n",
        "    pipe.text_encoder.resize_token_embeddings(len(pipe.tokenizer))\n",
        "new_token_id = pipe.tokenizer.convert_tokens_to_ids(new_token)\n",
        "\n",
        "# 3. Optimizer Tanımlaması (Öğrenme oranını 5e-5'e ayarlıyoruz)\n",
        "optimizer = optim.Adam([embedding_layer.weight], lr=5e-5)\n",
        "\n",
        "# 4. Noise Scheduler Oluşturma\n",
        "noise_scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "# 5. Eğitim Veri Kümesi Tanımlama\n",
        "class ConceptDataset(Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.image_paths = glob.glob(folder + \"/**/*.*\", recursive=True)\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize((512, 512)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Dataset yolunu kontrol edin: Dosyalar alt klasörlerde ise doğru yolu verin.\n",
        "dataset_folder = \"/content/dataset/Portraits_update/Portraits\"\n",
        "dataset = ConceptDataset(dataset_folder, transform=transform)\n",
        "\n",
        "# Batch size'ı 4'e düşürerek bellek kullanımını azaltıyoruz.\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "print(f\"Toplam {len(dataset)} görsel bulundu.\")\n",
        "\n",
        "# 6. AMP için GradScaler Oluşturma\n",
        "scaler = GradScaler()\n",
        "\n",
        "# 7. GERÇEK TEXTUAL INVERSION EĞİTİM DÖNGÜSÜ (TRAINING LOOP)\n",
        "num_epochs = 3  # Başlangıç olarak 3 epoch\n",
        "print(\"Textual inversion eğitimi başlıyor...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step, images in enumerate(dataloader):\n",
        "        # Görüntüleri fp16'ya çevirip cihazda tutuyoruz.\n",
        "        images = images.to(device).half()\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        # VAE kullanarak latent'leri elde ediyoruz.\n",
        "        with torch.no_grad():\n",
        "            latents = pipe.vae.encode(images).latent_dist.sample() * 0.18215\n",
        "\n",
        "        noise = torch.randn_like(latents)\n",
        "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=device).long()\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        prompt = f\"a photo of {new_token}\"\n",
        "        tokenized = pipe.tokenizer([prompt] * batch_size, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # AMP: Text encoder forward pass\n",
        "        with autocast():\n",
        "            text_embeddings = pipe.text_encoder(tokenized.input_ids)[0]\n",
        "            noise_pred = pipe.unet(noisy_latents, timesteps, text_embeddings).sample\n",
        "            loss = ((noise_pred - noise) ** 2).mean()\n",
        "\n",
        "        # Backward pass with scaling\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(embedding_layer.weight, max_norm=1.0)\n",
        "\n",
        "        # Yalnızca new_token_id satırına ait gradientleri korumak için maske uyguluyoruz\n",
        "        with torch.no_grad():\n",
        "            grad = embedding_layer.weight.grad\n",
        "            mask = torch.zeros_like(grad)\n",
        "            mask[new_token_id] = 1.0\n",
        "            embedding_layer.weight.grad.copy_(grad * mask)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 20 == 0:  # Logging sıklığını 20 adımda bir ayarladık\n",
        "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} tamamlandı.\")\n",
        "    # Epoch sonunda belleği temizleyelim.\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Textual inversion eğitimi tamamlandı.\")\n",
        "\n",
        "# 8. Eğitilmiş Token'ı Test Etme\n",
        "# Test aşaması için text encoder'ı FP16'ya çeviriyoruz\n",
        "with torch.no_grad():\n",
        "    pipe.text_encoder.half()\n",
        "\n",
        "pipe.unet.eval()\n",
        "test_prompt = f\"a photo of {new_token}\"\n",
        "result = pipe(test_prompt, guidance_scale=7.5)\n",
        "\n",
        "# Sonuç görselini kaydedelim\n",
        "result_image = result.images[0]\n",
        "result_image.save(\"textual_inversion_test_result.png\")\n",
        "result_image.show()\n"
      ],
      "metadata": {
        "id": "hfPies6kDwm-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    pipe.text_encoder.half()\n",
        "\n",
        "pipe.unet.eval()\n",
        "test_prompt = f\"a photo of {new_token}\"\n",
        "result = pipe(test_prompt, guidance_scale=7.5)\n",
        "\n",
        "# Sonuç görselini kaydedelim\n",
        "result_image = result.images[0]\n",
        "result_image.save(\"textual_inversion_test_result.png\")\n",
        "result_image.show()\n"
      ],
      "metadata": {
        "id": "rDF6gLfZGBod",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "display(result_image)\n"
      ],
      "metadata": {
        "id": "j92e2xIfGGRm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Aşağıdaki device, textual inversion eğitiminde kullandığımız device ile aynı olmalı.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 1. ControlNet Modelini Yükleyelim (inpainting için)\n",
        "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16)\n",
        "pipe_inpaint = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-inpainting\",\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe_inpaint = pipe_inpaint.to(device)\n",
        "\n",
        "# 2. Önceden üretilen görseli yükleyelim (textual inversion sonucu)\n",
        "input_image = Image.open(\"textual_inversion_test_result.png\").convert(\"RGB\")\n",
        "input_image.save(\"inpaint_input.png\")\n",
        "\n",
        "# 3. Inpainting için bir maske oluşturalım (örneğin, resmin ortasını maskeleyelim)\n",
        "img_np = np.array(input_image)\n",
        "mask_np = np.zeros_like(img_np)\n",
        "h, w, _ = mask_np.shape\n",
        "mask_np[h//4: 3*h//4, w//4: 3*w//4, :] = 255  # Beyaz alan, düzenlenecek bölge\n",
        "mask_image = Image.fromarray(mask_np)\n",
        "mask_image.save(\"inpaint_mask.png\")\n",
        "\n",
        "# 4. Inpainting prompt'ı: Eğitilmiş token'ı kullanarak düzenleme isteği\n",
        "inpaint_prompt = f\"a photo of <my_concept> with detailed enhancements in the masked area\"\n",
        "\n",
        "# 5. Inpainting işlemini gerçekleştir\n",
        "inpaint_result = pipe_inpaint(\n",
        "    prompt=inpaint_prompt,\n",
        "    image=input_image,\n",
        "    mask_image=mask_image,\n",
        "    control_image=input_image,  # Kontrol görüntüsü olarak giriş görselini kullanıyoruz\n",
        "    guidance_scale=7.5\n",
        ")\n",
        "\n",
        "# 6. Sonuç görselini kaydedip gösterelim\n",
        "inpaint_result.images[0].save(\"controlnet_inpaint_result.png\")\n",
        "display(inpaint_result.images[0])\n"
      ],
      "metadata": {
        "id": "8LYQ5saCHHOQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"generated_before\", exist_ok=True)\n",
        "os.makedirs(\"generated_after\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "FwhNQHDLHRbP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning öncesi üretilen görsel (örnek)\n",
        "result_before = pipe(\"a photo of <my_concept>\", guidance_scale=7.5)\n",
        "result_image_before = result_before.images[0]\n",
        "result_image_before.save(\"generated_before/image1.png\")\n"
      ],
      "metadata": {
        "id": "LJgMNVwKIC64",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning sonrası üretilen görseli oluşturma ve kaydetme\n",
        "test_prompt = f\"a photo of <my_concept>\"  # Eğitilmiş yeni token'ı kullanıyoruz\n",
        "result_after = pipe(test_prompt, guidance_scale=7.5)\n",
        "result_image_after = result_after.images[0]\n",
        "result_image_after.save(\"generated_after/image1.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UPTaafN8IeeU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerekli kütüphaneyi yükleyin\n",
        "!pip install pytorch-fid\n",
        "\n",
        "import torch\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dir_before = \"generated_before\"   # Fine-tuning öncesi görsellerin bulunduğu klasör\n",
        "dir_after = \"generated_after\"     # Fine-tuning sonrası görsellerin bulunduğu klasör\n",
        "\n",
        "fid_value = fid_score.calculate_fid_given_paths([dir_before, dir_after], batch_size=50, device=device, dims=2048)\n",
        "print(\"FID Skoru:\", fid_value)\n"
      ],
      "metadata": {
        "id": "r9ZGTIq5Ii6g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "id": "v8by2lq6JOCw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_image(prompt: str):\n",
        "    result = pipe(prompt, guidance_scale=7.5)\n",
        "    return result.images[0]\n",
        "\n",
        "example_prompts = [\n",
        "    \"A classic portrait painted with expressive brush strokes; detailed and realistic, featuring soft and dramatic lighting, a rich warm color palette, emotional expressions, and elegant textures.\",\n",
        "    \"An abstract, modern-style portrait rendered with dynamic brush strokes; featuring bold color contrasts, impressive light play, unique textures, and a contemporary artistic interpretation.\",\n",
        "    \"An exquisite, classical portrait meticulously painted with delicate, layered brush strokes that capture timeless beauty. The subject's refined features and expressive eyes exude quiet dignity, illuminated by soft, ethereal natural lighting. A warm, rich color palette with deep earth tones and subtle highlights imbues the scene with nostalgia and elegance, reminiscent of masterful Renaissance techniques.\",\n",
        "    \"An avant-garde, modern portrait characterized by dynamic, energetic brush strokes and bold, abstract forms. The image features a vivid interplay of stark, contrasting colors and dramatic lighting effects that create a sense of movement and intensity. Blending innovative digital techniques with traditional art aesthetics, this high-resolution artwork boasts intricate textures and a visually captivating composition that pushes the boundaries of contemporary portraiture.\",\n",
        "\n",
        "]\n",
        "\n",
        "case_explanation = \"\"\"\n",
        "### Process Overview\n",
        "\n",
        "**Model Setup:**\n",
        "The CompVis/stable-diffusion-v1-4 model was loaded in fp16 mode via the `diffusers` library and transferred to the GPU. A baseline image was generated using the prompt \"a portrait of a person\" and saved as *basic_generation.png*.\n",
        "\n",
        "**Fine-Tuning via Textual Inversion:**\n",
        "A new token `<my_concept>` was added to the tokenizer and integrated into the text encoder’s input embeddings. During fine-tuning, all parameters of UNet, VAE, and the remaining text encoder components were frozen; only the embedding corresponding to `<my_concept>` was optimized using a specialized portrait dataset over 1 epoch (~1628 steps). The fine-tuned output was saved as *textual_inversion_result.png*.\n",
        "\n",
        "**Advanced Techniques - ControlNet Inpainting:**\n",
        "The fine-tuned image was further refined using ControlNet inpainting. A mask was applied to the central area of the image, and the inpainting pipeline was executed with `<my_concept>` included in the prompt. The final result was saved as *controlnet_inpaint_result.png*.\n",
        "\n",
        "**Performance Evaluation:**\n",
        "The generated images were quantitatively compared to real portrait images using the Fréchet Inception Distance (FID) metric, which was calculated to be approximately 189.20. This indicates that further fine-tuning and hyperparameter optimization may be needed to enhance image fidelity.\n",
        "\"\"\"\n",
        "\n",
        "custom_css = \"\"\"\n",
        "@import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;700&display=swap');\n",
        "\n",
        "body {\n",
        "    background: linear-gradient(135deg, #141e30, #243b55);\n",
        "    font-family: 'Montserrat', sans-serif;\n",
        "    margin: 0;\n",
        "    padding: 0;\n",
        "}\n",
        "\n",
        ".container {\n",
        "    background: rgba(255, 255, 255, 0.95);\n",
        "    border: 1px solid #dfe6e9;\n",
        "    border-radius: 15px;\n",
        "    padding: 40px;\n",
        "    margin: 50px auto;\n",
        "    max-width: 950px;\n",
        "    box-shadow: 0 8px 30px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        "h1 {\n",
        "    color: #2c3e50;\n",
        "    text-align: center;\n",
        "    font-size: 3rem;\n",
        "    margin-bottom: 20px;\n",
        "}\n",
        "\n",
        "h2 {\n",
        "    /* Gradient blue-green text effect */\n",
        "    background: linear-gradient(to right, #007acc, #00a896);\n",
        "    -webkit-background-clip: text;\n",
        "    -webkit-text-fill-color: transparent;\n",
        "    text-align: center;\n",
        "    font-size: 2rem;\n",
        "    margin-bottom: 20px;\n",
        "}\n",
        "\n",
        "p, li {\n",
        "    color: #7f8c8d;\n",
        "    font-size: 1.1rem;\n",
        "    line-height: 1.6;\n",
        "}\n",
        "\n",
        ".gradio-container {\n",
        "    background: transparent;\n",
        "    border: none;\n",
        "}\n",
        "\n",
        "input, textarea {\n",
        "    border: 1px solid #bdc3c7;\n",
        "    border-radius: 8px;\n",
        "    padding: 12px;\n",
        "    width: 100%;\n",
        "    box-sizing: border-box;\n",
        "}\n",
        "\n",
        "button {\n",
        "    /* Blue-green gradient background for a professional look */\n",
        "    background: linear-gradient(45deg, #007acc, #00a896);\n",
        "    color: #ffffff;\n",
        "    border: none;\n",
        "    border-radius: 8px;\n",
        "    padding: 12px 20px;\n",
        "    font-size: 1rem;\n",
        "    font-weight: 600;\n",
        "    cursor: pointer;\n",
        "    transition: background 0.3s ease;\n",
        "}\n",
        "\n",
        "button:hover {\n",
        "    background: linear-gradient(45deg, #00a896, #007acc);\n",
        "}\n",
        "\n",
        "footer {\n",
        "    text-align: center;\n",
        "    margin-top: 30px;\n",
        "    font-size: 0.9rem;\n",
        "    color: #999999;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<div class='container'>\")\n",
        "    gr.Markdown(\"<h1>Welcome to Lumina Portraits</h1>\")\n",
        "    gr.Markdown(\"<p style='text-align:center; font-size:1.2rem; color:#2c3e50;'>Experience the future of portrait generation with our state-of-the-art fine-tuned Stable Diffusion model.</p>\")\n",
        "    gr.Markdown(\"<h2>Stable Diffusion Fine-Tuned Showcase</h2>\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Image Generation\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    prompt_input = gr.Textbox(lines=3, placeholder=\"Type your prompt here...\", label=\"Prompt\")\n",
        "                    generate_btn = gr.Button(\"Generate Image\")\n",
        "                with gr.Column(scale=3):\n",
        "                    image_output = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
        "            generate_btn.click(fn=generate_image, inputs=prompt_input, outputs=image_output)\n",
        "            gr.Markdown(\"<h2>Example Prompts</h2>\")\n",
        "            gr.Examples(\n",
        "                examples=example_prompts,\n",
        "                inputs=prompt_input,\n",
        "            )\n",
        "        with gr.TabItem(\"Process Overview\"):\n",
        "            gr.Markdown(case_explanation)\n",
        "    gr.Markdown(\"</div>\")\n",
        "    gr.Markdown(\"<footer>© 2025 Aurora Portraits - All Rights Reserved.</footer>\")\n",
        "\n",
        "demo.launch()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OQ7a7A13JjGK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}